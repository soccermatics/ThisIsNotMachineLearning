


\section{ML as a Practice that Alters the Social Fabric}
\label{social fabric}
\begin{displayquote}
``Technology is not the design of physical things. It is the design of practices and possibilities.'' Lucy Suchman \cite{suchman2007human}
\end{displayquote}

Machine classification and prediction are practices that act directly upon the world and result in tangible impact \cite{mcquillan2018data}. Various companies, institutes, and governments use machine learning systems across a variety of areas. These systems process people's behaviours, actions, and the social world, at large. The machine-detected patterns often provide ``answers'' to fuzzy, contingent, and open-ended questions. These ``answers'' neither reveal any causal relations nor provide explanation on \textit{why} or \textit{how} \cite{pasquale2015black}. Crucially, the more socially complex a problem is, the less capable machine learning systems are of accurately or reliably classifying or predicting \cite{salganik2020measuring}. %Yet, inferring and predicting human behaviour and actions based on patterns discerned from huge volumes of data has become common place. In a world marked by complexity, change, and uncertainty, shortcuts and simple answers are often championed \cite{birhane2020feb}. The practice of sorting, classifying, and predicting using machine learning tools is often applauded as a beacon of technological progress and a revolutionary marvel that provides answers to long standing problems. 
Yet, analytics companies boast their ability to provide insight into the human psyche and predict human behaviour \cite{Qualtrics}. Some even go so far as to claim to have built AI systems that are able to map and predict ``human states'' based on speech analysis, images of faces, and other data \cite{Affectiva}.

Thinking in relational terms about ethics begins with %the move away from conceiving data science and computation as mere methods and tools that cluster similarities and predict outcomes to 
reconceptualizing data science and machine learning as practices that create, sustain, and alter the social world. %Classification, pattern recognition, and prediction are, by and large, assumed to be technical and methodological problems that can be ``solved'' within the data since and ML communities. 
The very declaration of a taxonomy brings some things into existence while rendering others invisible \cite{bowker2000sorting}. For any individual person, community, or situation, algorithmic classifications and predictions give either an advantage or they hinder. Certain patterns are made visible and types of being objectified while other types are erased. Some identities (and not others) are recognised as a pedestrian \cite{wilson2019predictive}, or fit for a STEM career \cite{lambrecht2019algorithmic}, or in need of medical care \cite{obermeyer2019dissecting}. Some are ignored and made invisible altogether. 

% Gender classification system that conforms to essentialist binaries, for example, operationalizes gender in a cis-centric and gender non-binary and trans-exclusive way resulting in disproportionate harm to trans people \cite{keyes2018misgendering,hamidi2018gender}. 
Categories cut and demarcate boundaries. They simplify and freeze nuanced and complex narratives obscuring political and moral reasoning behind a category. Over time, messy and contingent histories and political and moral stories hidden behind a category are forgotten and trivialized \cite{star2007enacting}. The process of categorizing, sorting, and generalizing, therefore, is far from a mere technical task. While seemingly invisible in our daily lives, categorization and prediction bring forth some behaviours and ways of being  as ``legitimate'', ``standard'', or ``normal'' while casting others as ``deviant''\cite{star2007enacting}. Seemingly banal tasks such as identifying and predicting ``employable'' or ``criminal'' characteristics carry grave consequences for those that do not conform to the status quo. 

Relational ethics encourages us to view data science in general, and the tasks of developing and deploying algorithmic tools that cluster and predict, as part of the practice of creating and reinforcing existing and historical inequalities and structural injustices. Therefore, in treating data science as a practice that alters the fabric of society, the data practitioner is encouraged to zoom out and ask such questions as `how might the deployment of a specific tool enable or constrain certain behaviours and actions?', `does the deployment of such a tool enable or limit possibilities, and for whom?' and `in the process of enabling some behaviours while constraining others, how might such a tool be encouraging/discouraging certain social discourse and norms?'


%Could something follow here like: And if this is true, the possibility also lies here to start transforming the status quo, and  challenging inequalities and injustices, maybe even abolishing them. By, I don't know, creatively playing with them, by questioning them in the very ways data science is being done, and so on. Are there any practical, real things that you see that data scientists can do to begin working in a relational ethical way, or is it just criticism (for now?)? 
