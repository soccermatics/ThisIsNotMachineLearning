
In this chapter we start with a look at methods

When ML systems ``pick up'' patterns and clusters, this often amounts to identifying historically and socially held norms, conventions, and stereotypes. Machine prediction of social behaviour, I argue, is not only erroneous but also presents real harm to those at the margins of society.




\section{Detecting bias}





\section{Methods for ``removing'' bias}



\section{Other bias stuff moved here}


to supposedly cleanse datasets of bias or to make a set of existing models ``ethical'' for the problems we are trying to grasp are deeply rooted, fluid, contingent, and complex. Neither is it a rationally and logically constructed ``theory of ethics'' which hypothesizes about morality in abstract terms. 


\section{Bias is not a deviation from the ``correct'' description}
\label{bias is not a deviation}

One of the characteristics of a rational worldview is the tendency to perceive things as relatively static. In a supposedly objective worldview, bias, injustice, and discrimination are (mis)conceived as being able to be \textit{permanently corrected}. The common phrase ``bias in, bias out'' captures this deeply ingrained reductive thinking. Although datasets are often part of the problem, this commonly held belief relegates deeply rooted societal and historical injustices, nuanced power asymmetries, and structural inequalities to mere datasets. The implication is that if one can ``fix'' a certain dataset, then the deeper problems disappear. When we see bias and discrimination, what we see is problems that have surfaced as a result of a field that has thoughtlessly inherited deeply rooted unjust, racist, and white supremacist histories and practices~\cite{birhane2021algorithmic}. As D'Ignazio and F. Klein \cite{d2020data} contend, \textit{``addressing bias in a dataset is a tiny technological Band-Aid for a much larger problem''}. Furthermore, underlying the idea of ``fixing'' bias is the assumption that there exists a single \textit{correct description} of reality where a deviation from it has resulted in bias. As we have seen in Section \ref{rationality}, the idea of a single correct description, theory, or approach is reminiscent of the rationalist tradition where \textit{the correct way} is often synonymous with the \textit{status quo}. The idea of bias as something that can be eliminated, so to speak, once and for all, is misleading and problematic. Even if one can suppose that bias in a dataset can be ``fixed'', what exactly are we fixing? What is the supposedly bias free tool being applied to? Is it going to result in net benefit or harm to marginalized communities? Is the supposedly ``bias free'' tool used to punish, surveil, and harm anyway? And in Kalluri \cite{kalluri2020don}'s words, ``how is AI shifting power'' from the most to the least privileged? Looking beyond biased datasets and into deeper structural issues, historical antecedents, and power asymmetries is imperative. 

The rationalist worldview and its underlying assumptions are pervasive and take various nuanced forms. Within the computation and data sciences, the propensity to view things as relatively static manifests itself in the tendency to formulate subjects of study (people, ethics, and complex social problems in general) in terms of problem$\,\to\,$solution. Not only are subjects of study that do not lend themselves to this formulation discarded but also, this tradition rests on a misconception that injustice, ethics, and bias are relatively static things that we can \textit{solve once and for all}. Concepts such as bias, fairness, and justice, however, are moving targets. As we have discussed in section 2.2, neither people nor the environment and context they are embedded in are static. What society deems fair and ethical changes over time and with context and culture. The concepts of fairness, justice, and ethical practice are continually shifting. It is possible that what is considered ethical currently and within certain domains for certain societies will not be perceived similarly at a different time, in another domain, or by a different society. This, however, is not a call to relativism but rather an objection to static and final answers in the face of fluid reality. Adopting relational ethics means that we view our understandings, proposed solutions, and definitions of bias, fairness, and ethics as partially-open. This partial openness allows for revision and reiteration in accordance with the dynamic development of such challenges. This also means that this work is never \textit{done}.  

%To cite Rediet's new paper
%Toward a Critical Technical Practice:
%Lessons Learned in Trying to Reform AI
%Philip E. Agre
%Weizenbaum 
%Understanding computers and cognition 
%Ria's paper 
%Sabelo's relational ethics 
%Embodied ethics: Levinasâ€™ gift for enactivism file:///C:/Users/abeba/Downloads/M%C3%A9tais,%20Fabrice%20&%20Mario%20Villalobos,%20Mario.%20Embodied%20ethics,%20Levinas%E2%80%99gift%20for%20enactivism.pdf 

